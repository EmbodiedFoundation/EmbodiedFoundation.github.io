<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Embodied Foundation Video Models">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Embodied Foundation Video Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/thu-ml/">
            TSAIL Group
          </a>
          <a class="navbar-item" href="https://embodiedfoundation.github.io/hrdt">
            H-RDT
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Embodied Video Foundation Model: Vidar & AnyPos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://yaofeng1998.github.io">Yao Feng</a>*<sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://stormy-cheese-458.notion.site/Hengkai-Tan-22a9b6e0b5bb801782edc3771ff3ba37">Hengkai Tan</a>*<sup>1</sup>,
            </span>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Project Co-lead</span>
            </div>
            <!-- <br> -->
            <span class="author-block">
              <a href="https://github.com/shhmxy2">Xinyi Mao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/huangshuhe">Shuhe Huang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://numbnutn.github.io/">Guodong Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiang-cd.github.io/">Chendong Xiang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://haozhongkai.github.io/">Zhongkai Hao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao">Hang Su</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University</span><br>
            <!-- <span class="author-block"><sup>*</sup>Project Co-lead</span> -->
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span style="font-weight: bold;">ICML 2024</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2411.01850"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.12898"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Vidar arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.12768"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>AnyPos arXiv</span>
                </a>
              </span>

              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yaofeng1998/Vidar"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Vidar</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/EmbodiedFoundation/AnyPos"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>AnyPos</span>
                  </a>
              </span>

            <span class="link-block">
              <a href="https://huggingface.co/embodiedfoundation/Vidar"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon" style="font-size: 1.2em;">
                  ü§ó
                </span>
                <span>Vidar</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://huggingface.co/embodiedfoundation/AnyPos"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon" style="font-size: 1.2em;">
                  ü§ó
                </span>
                <span>AnyPos</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://huggingface.co/datasets/embodiedfoundation/ATARA"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon" style="font-size: 1.2em;">
                  ü§ó
                </span>
                <span>ATARA Dataset</span>
              </a>
            </span>
              
            <span class="link-block">
              <a href="https://x.com/Bernard99652469/status/1951981579842269485" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"> 
                  <i class="fab fa-twitter"></i>                            
                </span>
                <span>Twitter</span>
              </a>
            </span>  

            <span class="link-block">
              <a href="https://mp.weixin.qq.com/s/AGOcR7oiYSIQ6zbmFdswgw"
                  class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-external-link-alt"></i>
                </span>
                <span>ÈáèÂ≠ê‰ΩçÊé®ÈÄÅ(in Chinese)</span>
              </a>
            </span>

              <!-- <span class="link-block">
                <a href="https://zhuanlan.zhihu.com/p/15565509532"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-external-link-alt"></i>
                  </span>
                  <span>Áü•‰πéÊñáÁ´†(in Chinese)</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TODO: demo</h2>
      </div>
    </div>
  </div>
</section> -->

<div class="video-container mb-6" style="display: flex; justify-content: center;">
  <video controls autoplay muted loop width="100%" style="max-width: 800px;">
    <!-- <source src="{{ '/assets/videos/Vidar_AnyPos.mp4' | relative_url }}" type="video/mp4"> -->
    <source src="./assets/videos/demo_life_compressed.mp4"
                    type="video/mp4">
    <p>Your browser doesn't support HTML5 video. Here is a <a href="{{ '/assets/videos/demo.mp4' | relative_url }}">link to the video</a> instead.</p>
  </video>
</div>
<div class="video-container mb-6" style="display: flex; justify-content: center;">
  <video controls autoplay muted loop width="100%" style="max-width: 800px;">
    <!-- <source src="{{ '/assets/videos/Vidar_AnyPos.mp4' | relative_url }}" type="video/mp4"> -->
    <source src="./assets/videos/demo.mp4"
                    type="video/mp4">
    <p>Your browser doesn't support HTML5 video. Here is a <a href="{{ '/assets/videos/demo.mp4' | relative_url }}">link to the video</a> instead.</p>
  </video>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <item>
            <ul>
              <li>
                Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation.
              </li>
              <li>
                However, <strong>the heavy reliance on task-specific human demonstrations</strong> limits their generalization and incurs <strong>high data acquisition costs</strong>.
              </li>
              <li>
                We present a new notion of <strong><em>task-agnostic action </em></strong> paradigm that <strong>decouples action execution from task-specific conditioning</strong> to effectively overcome those limitations.
              </li>
              <li>
                To address the data collection challenges posed by this paradigm, we introduce <strong>ATARA</strong> (Automated Task-AgnosticRandom Actions), a novel data collection framework that automatically <strong>generates large-scale task-agnostic actions</strong> for bimanual manipulation efficiently. 
              </li>
              <li>
                By using a vision-generation model for future observation prediction and a downstream inverse dynamics model (IDM) for action regression, we can achieve <strong>exceptional generalization capability</strong> and <strong>remarkable data efficiency</strong>.
              </li>
              <li>
                <strong>Vidar</strong> (Video Diffusion for Action Reasoning) is a two-stage framework that leverages a large-scale, diffusion-based video pre-training model and a novel Masked Inverse Dynamics Model (MIDM) for action prediction, which can be generalized to an unseen robot platform <strong>with only 20 minutes of human demonstrations (1/81 of RDT demonstrations, 1/1200 of œÄ0.5 demonstrations)</strong>.
              </li>
              <li>
                <strong>AnyPos</strong> is another inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD) that <strong>is able to learn from ATARA-generated task-agnostic data</strong>. Therefore, ATARA and AnyPos constitute a fully task-agnostic framework for training IDMs <strong>without goal supervision</strong>.
              </li>
              <li>
                Our experiments demonstrate that our Vidar framework can generalize to unseen tasks and backgrounds with strong semantic understanding, surpassing state-of-the-art methods like VPP and UniPi by <strong>over 40%</strong>.
              </li>
              <li>
                We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks, and we demonstrate that the AnyPos-ATARA pipeline yields a <strong>51% improvement</strong> in test accuracy and achieves <strong>30-40% higher success rates</strong> in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation.
              </li>
            </ul>
          </item>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Vidar: Video Diffusion for Action Reasoning</h2>
        <!-- pic -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/pdfs/Vidar.png" alt="Vidar Architecture" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: Overview of Vidar.</p>
        </div>
        
        <div class="content has-text-justified">
          <p>
            Vidar includes a video diffusion foundation model for video prediction and a masked inverse dynamics model (MIDM) for action regression.
            The video diffusion model is trained on <strong>750K multi-view bimanual videos</strong> with test-time scaling (TTS) applied during testing, and it can adapt to new robot platforms <strong>with only 20 minutes of demonstrations</strong> with state-of-the-art performance and generalize to unseen tasks with strong semantic understanding.
          </p>
        </div>

        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/pdfs/Vidar_methods.png" alt="Vidar Methods" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: Methods of Vidar.</p>
        </div>

        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/pdfs/Vidar_examples.png" alt="Vidar Examples" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: Examples of Vidar.</p>
        </div>
        <!-- Key Techniques -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Key Techniques</h3>
          <ul>
            <li><strong>Video Generation Model:</strong> Rectified flow models with internet-videos pre-training, emboided pre-training and fine-tuning on unified observation space.</li>
            <li><strong>Masked Inverse Dynamics Model (MIDM):</strong> 
              Inverse dynamics models often suffer from poor generalization due to the presence of background noise, texture biases, and visual distractions in high-dimensional observation, while MIDM can focus on task-relevent regions of the input frame via implicit mask prediction.
            </li>
            <li><strong>Test-Time Scaling (TTS):</strong> We generate <em>K</em> candidate video trajectories using different random seeds. Then rank these trajectories using a pretrained evaluator (e.g., CLIP or a vision-language model) and select the one which scores the highest.</li>
          </ul>
        </div>
        
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Anypos: Automated Task-Agnostic Actions for Bimanual Manipulation</h2>

        <!-- pic -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/vidar_anypos/vgm_deploy_compare.png" alt="The objective of VLAs" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: From dream world prediction to real world execution.</p>
        </div>

        <!-- pic -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/vidar_anypos/anypos_header.png" alt="Overview of AnyPos" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: Overview of AnyPos.</p>
        </div>

        <div class="content has-text-justified">
          <p>
            AnyPos is a robot-specific image-to-action model <strong>trained entirely on task-agnostic trajectories sampled by ATARA</strong>. It integrates two key techniques to enhance performance: <strong>Arm-Decoupled Estimation</strong> and <strong>Direction-Aware Decoder (DAD)</strong>.
            Together, ATARA and AnyPos constitute <strong>a fully task-agnostic framework for training IDMs without goal supervision</strong>.
            By combining scalable unsupervised data collection with physically informed learning architectures, our approach demonstrates that task-agnostic action data can serve as a practical and powerful foundation for generalizable manipulation.
          </p>
          <p>
            Our AnyPos method achieves 57.13% action prediction accuracy on the test set, which includes unseen skills and objects, surpassing previous approaches (na√Øve ResNet+MLP used in unipi, unisim, robodreamer, and susie) by <strong>51%</strong>. In real-world robot replay tests, AnyPos-ATARA demonstrates 92.59% task success rate (as shown in the figure below), representing a <strong>33%</strong> improvement over the human-collected dataset, and surpassing previous approaches by 44% (as shown in the Figure of "Overview of AnyPos").
          </p>
        </div>
        
        <!-- pic -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/vidar_anypos/anypos_atara_replay.png" alt="The objective of VLAs" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: The results of AnyPos-ATARA with video replay to accomplish various manipulation tasks.</p>
        </div>
        
        <div class="content has-text-justified">
          <h3 class="title is-4">ATARA: Automatically collect data of target robots! (4x speed)</h3>
          <div class="video-container mb-6" style="display: flex; justify-content: center;">
            <video controls autoplay muted loop width="60%" style="max-width: 800px;">
              <!-- <source src="{{ '/assets/videos/Vidar_AnyPos.mp4' | relative_url }}" type="video/mp4"> -->
              <source src="./assets/videos/ATARA_episode_214_random_4x.mp4"
                              type="video/mp4">
              <p>Your browser doesn't support HTML5 video. Here is a <a href="{{ '/assets/videos/Vidar_AnyPos.mp4' | relative_url }}">link to the video</a> instead.</p>
            </video>
          </div>
        </div>
        
        <div class="content has-text-justified">
          <h3 class="title is-3">Task-Agnostic Action</h3>
          
          <div class="content has-text-justified">
            <p>
              Standard VLA models learn temporally extended policies 
              <math alttext="p_{\theta}(\bm{a}_{T+1:T+t}|\bm{x}_{T-H+1:T},\mathbf{l})" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.2"><semantics id="S3.SS1.p1.7.m7.2a"><mrow id="S3.SS1.p1.7.m7.2.2" xref="S3.SS1.p1.7.m7.2.2.cmml"><msub id="S3.SS1.p1.7.m7.2.2.3" xref="S3.SS1.p1.7.m7.2.2.3.cmml"><mi id="S3.SS1.p1.7.m7.2.2.3.2" xref="S3.SS1.p1.7.m7.2.2.3.2.cmml">p</mi><mi id="S3.SS1.p1.7.m7.2.2.3.3" xref="S3.SS1.p1.7.m7.2.2.3.3.cmml">Œ∏</mi></msub><mo id="S3.SS1.p1.7.m7.2.2.2" xref="S3.SS1.p1.7.m7.2.2.2.cmml">‚Å¢</mo><mrow id="S3.SS1.p1.7.m7.2.2.1.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.cmml"><mo id="S3.SS1.p1.7.m7.2.2.1.1.2" stretchy="false" xref="S3.SS1.p1.7.m7.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.cmml"><msub id="S3.SS1.p1.7.m7.2.2.1.1.1.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.3.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.2.cmml">ùíÇ</mi><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.cmml"><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.2.cmml">T</mi><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.1.cmml">+</mo><mn id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.1.cmml">:</mo><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.2.cmml">T</mi><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.1.cmml">+</mo><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.3.cmml">t</mi></mrow></mrow></msub><mo fence="false" id="S3.SS1.p1.7.m7.2.2.1.1.1.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.2.cmml"><msub id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.2.cmml">ùíô</mi><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.cmml"><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.cmml"><mrow id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.cmml"><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.2.cmml">T</mi><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.1.cmml">‚àí</mo><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.3.cmml">H</mi></mrow><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.1" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.1.cmml">+</mo><mn id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.3.cmml">1</mn></mrow><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.1" lspace="0.278em" rspace="0.278em" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.3.cmml">T</mi></mrow></msub><mo id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.2.cmml">,</mo><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">ùê•</mi></mrow></mrow><mo id="S3.SS1.p1.7.m7.2.2.1.1.3" stretchy="false" xref="S3.SS1.p1.7.m7.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><apply id="S3.SS1.p1.7.m7.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2"><times id="S3.SS1.p1.7.m7.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.2"></times><apply id="S3.SS1.p1.7.m7.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.3.2">ùëù</ci><ci id="S3.SS1.p1.7.m7.2.2.3.3.cmml" xref="S3.SS1.p1.7.m7.2.2.3.3">ùúÉ</ci></apply><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.p1.7.m7.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.2">conditional</csymbol><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.2">ùíÇ</ci><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3"><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.1">:</ci><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2"><plus id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.1"></plus><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.2">ùëá</ci><cn id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.3.cmml" type="integer" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.2.3">1</cn></apply><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3"><plus id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.1"></plus><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.2">ùëá</ci><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.3.3.3.3">ùë°</ci></apply></apply></apply><list id="S3.SS1.p1.7.m7.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1"><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.2">ùíô</ci><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3"><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.1">:</ci><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2"><plus id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.1"></plus><apply id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2"><minus id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.1"></minus><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.2">ùëá</ci><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.2.3">ùêª</ci></apply><cn id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.3.cmml" type="integer" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.2.3">1</cn></apply><ci id="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.2.2.1.1.1.1.1.1.3.3">ùëá</ci></apply></apply><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">ùê•</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">p_{\theta}(\bm{a}_{T+1:T+t}|\bm{x}_{T-H+1:T},\mathbf{l})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.2d">italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_a start_POSTSUBSCRIPT italic_T + 1 : italic_T + italic_t end_POSTSUBSCRIPT | bold_italic_x start_POSTSUBSCRIPT italic_T - italic_H + 1 : italic_T end_POSTSUBSCRIPT , bold_l )</annotation></semantics></math>
              , where
              <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_Œ∏</annotation></semantics></math>
              represents the parameters of the VLA policy, T  is the current timestep and H  denotes the history window size, mapping observation histories and language commands to action sequences.
              Given an expert dataset <math alttext="D_{\text{expert}}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11.1"><semantics id="S3.SS1.p1.11.m11.1a"><msub id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml"><mi id="S3.SS1.p1.11.m11.1.1.2" xref="S3.SS1.p1.11.m11.1.1.2.cmml">D</mi><mtext id="S3.SS1.p1.11.m11.1.1.3" xref="S3.SS1.p1.11.m11.1.1.3a.cmml">expert</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><apply id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.11.m11.1.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p1.11.m11.1.1.2.cmml" xref="S3.SS1.p1.11.m11.1.1.2">ùê∑</ci><ci id="S3.SS1.p1.11.m11.1.1.3a.cmml" xref="S3.SS1.p1.11.m11.1.1.3"><mtext id="S3.SS1.p1.11.m11.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.11.m11.1.1.3">expert</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">D_{\text{expert}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.11.m11.1d">italic_D start_POSTSUBSCRIPT expert end_POSTSUBSCRIPT</annotation></semantics></math> 
              , the training objective of VLAs is to maximize the likelihood:
            </p>
  
            <!-- pic -->
            <div class="has-text-centered my-5">
              <figure class="image">
                <img src="./assets/vidar_anypos/vla_distri.png" alt="The objective of VLAs" style="max-width: 120%; height: auto;">
              </figure>
              <p class="has-text-grey is-size-6">Figure 3: The objective of VLAs.</p>
            </div>
          </div>
          
          
          <div class="content has-text-justified">
            Here, all actions are task-dependent, i.e., <strong>Task-Specific Actions</strong>. The vastness of task language instructions and action spaces creates an enormous demand for action data in Vision-Language-Action (VLA) models. <br>
            In scenarios where robotic actions are position-controlled, the above formulation can be decomposed into a "future video prediction problem" and an "action execution problem." This enables the <strong>decoupling of the action modality from the embodied foundation model</strong>, shifting the requirement for high generalizability to the data-rich vision-language modality. Through our derivation, we propose the concept of Task-Agnostic Action, which significantly simplifies the learning of the action modality:
          </div>
  
          <div class="has-text-centered my-5">
            <figure class="image">
              <img src="./assets/vidar_anypos/ours_distri.png" alt="Decompsing task-specific action." style="max-width: 120%; height: auto;">
            </figure>
            <p class="has-text-grey is-size-6">Figure 4: Decompsing task-specific action.</p>
          </div>
          <!-- Key Techniques -->
          <div class="content has-text-justified">
            <h3 class="title is-4">Benefits of task-agnostic action paradigm</h3>
            <ol>
              <li>
                <strong>Data Efficiency and Reusable Motor Skills</strong>: Task-agnostic training avoids costly task-specific demonstrations, enabling large-scale unsupervised data collection. The inverse dynamics model (IDM), which learns a universal action prior  <math alttext="p(\bm{a}_{i}\mid\bm{x}_{i})" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">p</mi><mo id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.SS1.p4.1.m1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS1.p4.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p4.1.m1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS1.p4.1.m1.1.1.1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.cmml">ùíÇ</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.2.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p4.1.m1.1.1.1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1.cmml">‚à£</mo><msub id="S3.SS1.p4.1.m1.1.1.1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.3.2" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.2.cmml">ùíô</mi><mi id="S3.SS1.p4.1.m1.1.1.1.1.1.3.3" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS1.p4.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2"></times><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">ùëù</ci><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.2">ùíÇ</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S3.SS1.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.2">ùíô</ci><ci id="S3.SS1.p4.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p4.1.m1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">p(\bm{a}_{i}\mid\bm{x}_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">italic_p ( bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>
                , acts as a shared motor skill library for diverse tasks.
              </li>
              <li><strong>Zero-Shot Task Generalization:</strong> 
                The IDM models task-independent action priors, allowing generalization to new tasks by adapting only the video generation model (e.g., via language prompts) without IDM retraining.
              </li>
              <li><strong>Decoupled Planning and Low-Level Control:</strong> 
                High-level planning (e.g., "open the drawer") is handled by a video generation model, while the IDM executes the visual trajectories. This modular approach simplifies policy design by framing manipulation as an vision-space prediction problem.
              </li>
            </ol>
          </div>
          
        </div>

        <!-- Key Techniques -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Key Techniques</h3>
          
        <!-- pic -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/vidar_anypos/anypos_methods.png" alt="Methods of AnyPos" style="max-width: 120%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure: Methods of AnyPos. We obtain a task-agnostic training dataset covering the entire cubic workspace of dual robotic arms using ATARA. </p>
        </div>
          <ul>
            <li><strong>ATARA:</strong> As shown in the Figure of "Methods of AnyPos", na√Øve joint-space sampling often results in inefficient coverage of reachable states, redundant or degenerate motions (e.g., arms exiting the field of view), and frequent self-collisions. To address these limitations, we propose ATARA, a reinforcement learning framework that constructs a coverage-aware mapping from end-effector space to joint space. Therefore, it enables efficient task-agnostic data generation that preserves inherently encoded robot embodiment information and broad behavioral coverage, serving as a reusable prior for downstream policy learning.</li>
            <li><strong>Arm-Decoupled Estimation:</strong> 
            We observe that when estimating left arm joints, the model often attends to visual features of the right arm, and vice versa. To mitigate this, we isolate the input features per arm. We (1) split the left and right arms in the observation <em>x</em>, then (2) use two sub-networks estimate joint positions for each arm independently. Gripper poses are estimated by specialized networks. This decoupling reduces the visual hypothesis space, improves estimation accuracy, and enables specialization per arm.
            </li>
            <li><strong>Direction-Aware Decoder (DAD):</strong> For action estimation networks, we choose DINOv2 with register (DINOv2-Reg) as the visual encoder, and uses three core componets to meet the high-precision requirement for action prediction: (1) Multi-Scale Dilated Convolutions (2) Deformable Convolutions (3) Angle-Sensitive Pooling</li>
          </ul>
        </div>


      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <hr>
<h3>Team (TODO)</h3>
<section>
  <div class="box alt" style="margin-bottom: 1em;">
    <div class="row 50% uniform" style="
    width: 80%; 
    display: flex;      /* Êñ∞Â¢û */
    flex-wrap: wrap;    /* ÂÖÅËÆ∏Êç¢Ë°å */
    justify-content: center; /* Ê∞¥Âπ≥Â±Ö‰∏≠ */
  ">
      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://cheng-chi.github.io/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Yao Feng<sup>*1,2</sup></a></div>
      
      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://zhenjiaxu.com/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Hengkai Tan<sup>*1,2</sup></a></div>
      
      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://chuerpan.com/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Xinyi Mao<sup>1</sup></a></div>

      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://www.eacousineau.com/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Shuhe Huang<sup>3</sup></a></div>
      
      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="http://www.benburchfiel.com/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Guodong Liu<sup>3</sup></a></div>

      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://www.cs.cmu.edu/~sfeng/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Chendong Xiang<sup>3</sup></a></div>

      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://www.cs.cmu.edu/~sfeng/"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Zhongkai Hao<sup>3</sup></a></div>
              
      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%">
        <a href="https://groups.csail.mit.edu/locomotion/russt.html"><span class="image fit" style="margin-bottom: 0.5em;">
          <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Hang Su<sup>3</sup></a></div>

      <div class="2u" style="font-size: 1em; line-height: 1.5em; text-align: center; width: 12.5%"
      ><a href="https://shurans.github.io/"><span class="image fit" style="margin-bottom: 0.5em;">
        <img src="favicon.ico" alt="" style="border-radius: 50%;" /></span>Jun Zhu<sup>1,2</sup></a></div>
      
    </div>
  </div>
</section> -->
<!-- <p>
  <sup>1</sup> Tsinghua University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <sup>2</sup>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <sup>3</sup>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <sup>4</sup> MIT&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
  <!-- <sup>*</sup> Indicates Co-Lead
</p> -->



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    If you find our work helpful, please cite us:
    <pre><code>
@misc{feng2025vidarembodiedvideodiffusion,
    title={Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation},
    author={Yao Feng and Hengkai Tan and Xinyi Mao and Guodong Liu and Shuhe Huang and Chendong Xiang and Hang Su and Jun Zhu},
    year={2025},
    eprint={2507.12898},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2507.12898}, 
}
@misc{tan2025anyposautomatedtaskagnosticactions,
    title={AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation}, 
    author={Hengkai Tan and Yao Feng and Xinyi Mao and Shuhe Huang and Guodong Liu and Zhongkai Hao and Hang Su and Jun Zhu},
    year={2025},
    eprint={2507.12768},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2507.12768}, 
}</code></pre>
    Thank you!
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2411.01850">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/thkkk" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 Intsernational License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>