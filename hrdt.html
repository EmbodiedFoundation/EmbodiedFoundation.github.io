<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation">
  <meta name="keywords" content="H-RDT, Robot Learning, Imitation Learning, Diffusion Transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/thu-ml/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://hongzhebi.github.io/">Hongzhe Bi</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
                <a href="https://ethan-iai.github.io/">Lingxuan Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="https://wzmsltw.github.io/">Tianwei Lin</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=HQfc8TEAAAAJ&hl=en">ZhiZhong Su</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao">Hang Su</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml">Jun Zhu</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University</span><br>
            <span class="author-block"><sup>2</sup>Horizon Robotics</span><br>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Imitation learning for robotic manipulation faces a fundamental challenge: <strong>the scarcity of large-scale, high-quality robot demonstration data</strong>.
          </p>
          <ul>
            <li>
              While recent robotic foundation models often pre-train on cross-embodiment robot datasets, they face significant limitations: the diverse morphologies and action spaces across different robot embodiments make unified training challenging.
            </li>
            <li>
              We present <strong>H-RDT</strong> (<strong>Human to Robotics Diffusion Transformer</strong>), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities.
            </li>
            <li>
              Our key insight is that <strong>large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors</strong> that capture natural manipulation strategies and can benefit robotic policy learning.
            </li>
            <li>
              We introduce a two-stage training paradigm: <strong>(1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders</strong>.
            </li>
            <li>
              Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions.
            </li>
            <li>
              The modular design of action encoder and decoder components enables effective knowledge transfer from the unified human embodiment to diverse robot platforms through efficient fine-tuning.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Performance Overview Figure -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/figure1.png" alt="H-RDT Performance Overview" style="max-width: 80%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure 1: Performance overview on RoboTwin 2.0 multi-task evaluations and few-shot real-world experiments.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        
        <!-- Architecture Overview -->
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/hrt_architecture.png" alt="H-RDT Architecture" style="max-width: 85%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure 2: H-RDT architecture overview.</p>
        </div>
        
        <div class="content has-text-justified">
          <h3 class="title is-4">Key Innovations</h3>
          <ul>
            <li>
              <strong>Data Scarcity Solution</strong>: We harness the abundance of human manipulation videos with 3D hand pose annotations to provide rich behavioral priors that capture natural manipulation strategies, object affordances, and task decomposition patterns.
            </li>
            <li>
              <strong>Cross-Embodiment Transfer</strong>: We develop a modular transformer architecture with specialized action encoders and decoders that enables effective knowledge transfer from human demonstrations to diverse robotic platforms while preserving learned manipulation knowledge.
            </li>
            <li>
              <strong>Training Efficiency</strong>: We employ a two-stage training paradigm, pre-training on large-scale human data followed by cross-embodiment fine-tuningâ€”combined with flow matching for stable and efficient policy learning.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experimental Results -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        
        <h3 class="title is-4">Simulation Results on RoboTwin 2.0</h3>
        
        <h4 class="title is-5">Single-Task Performance</h4>
        <div class="content has-text-justified">
          <p>
            We evaluate single-task performance on 13 representative manipulation tasks from the RoboTwin 2.0 benchmark. Each task is trained on 50 demonstrations and evaluated in Easy mode (clean environments) and Hard mode (domain randomization).
          </p>
        </div>
        
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/table/table1.png" alt="Single-Task Results" style="max-width: 90%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Table 1: Single-task success rates (%) on RoboTwin 2.0 benchmark. H-RDT achieves the highest average performance across both Easy and Hard evaluation modes.</p>
        </div>

        <h4 class="title is-5">Multi-Task Performance</h4>
        <div class="content has-text-justified">
          <p>
            We conduct multi-task experiments on 45 tasks from RoboTwin 2.0, training on approximately 2250 demonstrations collected under domain randomization (Hard mode data).
          </p>
        </div>
        
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/table/table2.png" alt="Multi-Task Results" style="max-width: 80%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Table 2: Multi-task success rates (%) on RoboTwin 2.0 benchmark (Hard mode evaluation). H-RDT achieves 87.2% average success rate, significantly outperforming all baselines.</p>
        </div>

        <h4 class="title is-5">Cross-Embodiment Generalization</h4>
        <div class="content has-text-justified">
          <p>
            To validate cross-embodiment transfer capabilities, we conduct multi-task experiments across different robotic embodiments in simulation, evaluating both Aloha-Agilex-1.0 and Franka-Panda platforms.
          </p>
        </div>
        
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/figure3.png" alt="Cross-Embodiment Results" style="max-width: 75%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure 3: Cross-embodiment multi-task performance on RoboTwin 2.0 tasks. H-RDT demonstrates strong performance across different robotic morphologies.</p>
        </div>

        <h3 class="title is-4">Real-world Validation</h3>
        
        <h4 class="title is-5">Dual-arm ARX5 Few-shot Experiments</h4>
        <div class="content has-text-justified">
          <p>
            We design a challenging real-world experiment with 113 diverse pick-and-place tasks using a dual-arm ARX5 robotic system, with only 1-5 demonstrations per task. This extreme multi-task few-shot setting tests the limits of sample efficiency.
          </p>
        </div>
        
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/table/table3.png" alt="Real-world Few-shot Results" style="max-width: 85%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Table 3: Real-world few-shot learning results on manipulation tasks. Each task has 1-5 demonstrations for training. H-RDT demonstrates superior sample efficiency with 41.6% average success rate.</p>
        </div>

        <h4 class="title is-5">Cross-Platform Real-world Results</h4>
        <div class="content has-text-justified">
          <p>
            We further validate H-RDT across multiple real-world robotic platforms to demonstrate cross-embodiment transfer capabilities and robustness in practical deployment scenarios.
          </p>
        </div>
        
        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/table/table5.png" alt="Cross-Platform Results" style="max-width: 85%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Table 4: Cross-platform real-world validation results showing H-RDT's effectiveness across different robotic embodiments.</p>
        </div>

        <div class="has-text-centered my-5">
          <figure class="image">
            <img src="./assets/hrdt/figure2.png" alt="H-RDT Analysis" style="max-width: 75%; height: auto;">
          </figure>
          <p class="has-text-grey is-size-6">Figure 4: Detailed performance analysis and comparison across different experimental settings.</p>
        </div>
        
        <div class="content has-text-justified">
          <h3 class="title is-4">Key Findings</h3>
          <ul>
            <li>
              <strong>Simulation Excellence</strong>: H-RDT achieves 68.7% success rate in single-task and 87.2% in multi-task scenarios, substantially outperforming existing methods.
            </li>
            <li>
              <strong>Real-world Transfer</strong>: Demonstrates remarkable resilience in extreme few-shot settings (1-5 demonstrations), achieving 41.6% success rate compared to 31.2% for Pi-0.
            </li>
            <li>
              <strong>Cross-Embodiment Robustness</strong>: Consistent improvements across different robotic morphologies validate the cross-embodiment generalization capabilities.
            </li>
            <li>
              <strong>Human Prior Benefits</strong>: The substantial improvements over training from scratch validate our core hypothesis that human manipulation data provides valuable inductive biases.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <strong>Novel Human-to-Robot Knowledge Transfer Framework</strong>: Structural and training method innovations for human-to-robot knowledge transfer through human manipulation data pre-training.
            </li>
            <li>
              <strong>Modular Diffusion Transformer Architecture</strong>: Specialized action encoders and decoders designed to support effective knowledge transfer from human embodiment to diverse robot platforms.
            </li>
            <li>
              <strong>Two-Stage Training Strategy</strong>: Combining large-scale human data pre-training with cross-embodiment fine-tuning using flow matching techniques.
            </li>
            <li>
              <strong>Comprehensive Experimental Validation</strong>: Demonstrating effectiveness across multiple tasks, environments, and evaluation metrics.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html> 